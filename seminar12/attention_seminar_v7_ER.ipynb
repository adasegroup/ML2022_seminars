{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/adasegroup/ML2022_seminars/blob/master/seminar12/attention_seminar_v7_ER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kfkeWFbalNxg"
   },
   "source": [
    "# Attention mechanism from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kFsFrRYTWsGr"
   },
   "source": [
    "## Recap\n",
    "\n",
    "\n",
    "* Before the attention mechanism appeared, if we want to solve a machine translation problem (or any seq-seq problem), we use RNN to compute some representation of a sequence using the encoder and then predict sequence by different RNN using obtained representations.  **Bottleneck problem!**\n",
    "\n",
    "![machine-transaction.PNG](https://raw.githubusercontent.com/zaaabik/msd2021/main/machine-transaction.PNG)\n",
    "\n",
    "**Soltution:** to reduce bottleneck problem, we could somehow connect predicted token not only with the previous token but with all elements from the encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_j5MekUtf-B"
   },
   "source": [
    "\n",
    "\n",
    "![attention-machine-translation](https://raw.githubusercontent.com/zaaabik/msd2021/main/attention.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CUcjet09vzAy"
   },
   "source": [
    "### Key concepts of attention \n",
    "\n",
    "Attention is based on three core objects **query, key, value**.\n",
    "\n",
    "Attention principles are very naturals and could be interpreted as:\n",
    "\n",
    "1. Query - what you want to find (decoder)\n",
    "2. Key - representation of data (encoder)\n",
    "3. Value - the importance of the object in data (encoder)\n",
    "\n",
    "The query is what you want to find in a date, key - represents data for query search, and value is information stored in an object. A natural example could be illustrated in the machine translation task. \n",
    "\n",
    "![language-model.png](https://raw.githubusercontent.com/zaaabik/msd2021/main/attention_sentence.png)\n",
    "\n",
    "In this picture, queries from one language have bigger attention for words with the same meaning but in a different language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i5SXRPxMxLHr"
   },
   "source": [
    "## Lets go into details!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0DOwOA5Dc6eC"
   },
   "source": [
    "### Similarity coefficient\n",
    "First of all, we need to compute the similarity coefficient between each question and key. \n",
    "\n",
    "\\begin{align}\n",
    " cᵢⱼ = f(qᵢ, kⱼ) \n",
    "\\end{align}\n",
    "where $c$ is similairty coefficient between $i$-query with $j$ key.\n",
    "\n",
    "Function $f$ could be an arbitrary function that returns the real number and describes two elements' similarities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zqqKMLpkcLxj"
   },
   "source": [
    "### Attention weights\n",
    "For each $ q_i $, we compute similarity coefficient with whole keys and obtain vector from $ R^{n} $, where n - number of keys.\n",
    "\\begin{align}\n",
    " C_i  = [c_{i 1}, c_{i 2} ... c_{i n}]\n",
    "\\end{align}\n",
    "\n",
    "One of the problems is that elements in $ C_i $ could be on a different scale. So, we want to map the sum into $(0, 1)$ and interpret elements as a probability. To do it, we use the softmax function:\n",
    "\n",
    "\\begin{align}\n",
    "a_{ij} = \\frac{\\text{exp}(c_{i j})}{\\sum_n \\text{exp}(a_{in})}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "A_i  = softmax(C_i) = softmax([c_{i 1}, c_{i 2} ... c_{i n}])\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tltZ0D91krHr"
   },
   "source": [
    "### Attention matrix\n",
    "Finally we get an attemtion matrix, where each row is softmaxed similarity coefficent of one query with all keys.\n",
    "\n",
    "\\begin{align}\n",
    "A_i  = [\n",
    "    A_1,\\\\\n",
    "    A_2,\\\\\n",
    "    ...\\\\\n",
    "    A_{n - 1}\\\\\n",
    "    A_n] \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DuwPOrzMkKHn"
   },
   "source": [
    "### Attention output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tFM6Cu9JnS5z"
   },
   "source": [
    "The output of the attention block is a weighted sum of values.\n",
    "\\begin{align}\n",
    " O_k = \\sum^n_{i = 1} a_{k i} v_i = A_k V\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8IqBRFdOE_67"
   },
   "source": [
    "## Lets code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Aya_zRVdFXmC"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uaE8xOg1gEh0"
   },
   "source": [
    "### Similarity coefficient\n",
    "We should create a function that takes two vectors with the same shape and return a real value. The most straightforward measure is the dot product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a-p_qJnJKBUb"
   },
   "source": [
    "### Dot product attention\n",
    "The core idea is to use a scaled dot product as a similarity coefficient.\n",
    "\n",
    "\\begin{align}\n",
    "c  =  \\frac{\\langle \\mathbf{q}, \\mathbf{k} \\rangle}{\\sqrt{d}}\n",
    "\\end{align}\n",
    "The function takes two vectors with the same dimension $q, k \\in R^d$, and compute dot product and device by dimension size. Dividing helps reduce std difference for vectors with different size and make the function more general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E11rI5CeFfj-"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "d = 256\n",
    "x = torch.rand(d)\n",
    "y = torch.rand(d)\n",
    "\n",
    "def scalar_similarity_coefficient(x,y, d):\n",
    "    ## TODO write scaled dot product\n",
    "    return torch.dot(x,y) / np.sqrt(d)\n",
    "\n",
    "similarity_score = scalar_similarity_coefficient(x,y, d)\n",
    "assert(\n",
    "    torch.allclose(similarity_score, \n",
    "                   torch.tensor(3.854549))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KlWzA5xW6ngw"
   },
   "source": [
    "But there are many different functions we can use as a score function:\n",
    "\n",
    "\n",
    "1.   Multilayer Perceptron Attention\n",
    "\\begin{align}\n",
    "c = \\mathbf{v}^T \\text{tanh}(\\mathbf{W}_k \\mathbf{k} + \\mathbf{W}_q \\mathbf{q}).\n",
    "\\end{align}\n",
    "2.   Neural network\n",
    "\\begin{align}\n",
    "c = \\text{NN}(q \\mathbin\\Vert k)\n",
    "\\end{align} where $q \\mathbin\\Vert k$ - concatenating the key and value in the feature dimension\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z6lGJ_WLH5Dy"
   },
   "source": [
    "To compute coefficient for multiple queries and keys efficient, we need to rewrite all in **vectorized form**.\n",
    "\n",
    "We assume  $q \\in R^{b, t, d}$ and $ k \\in R^{b, k, d} $ where $b$ - batch size, $t$ and $k$ - number of queries and keys, $d$ - the shape of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nhZlnAGvevq3"
   },
   "source": [
    "PyTorch framework has the function for multiplication elements in a batch manner. \n",
    "\n",
    "`torch.bnn(tensor1, tensor2)`\n",
    ", where tensor1 is a $ (b \\times n \\times m) $ and tensor2 is a $ (b \\times m \\times p) $ . This function produce matrix multiplication for each object in a batch independent and output has shape $(b \\times  n \\; x \\times p)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RSa4f-gEI-nh"
   },
   "outputs": [],
   "source": [
    "## imagine that we have 10 queries and 8 keys and need to construce matrix of cofficent.\n",
    "\n",
    "torch.manual_seed(0)\n",
    "d = 64\n",
    "t,k = 10, 8\n",
    "batch_size = 3\n",
    "query = torch.rand(10, d)\n",
    "query = query.repeat(batch_size, 1, 1)\n",
    "\n",
    "key = torch.rand(8, d)\n",
    "key = key.repeat(batch_size, 1, 1)\n",
    "\n",
    "def compute_coefficient_matrix(query, key, d):\n",
    "    ## TODO write vectorized scaled dot product \n",
    "    return query @ key.transpose(-1,-2) / np.sqrt(d)\n",
    "\n",
    "coefficient_matrix = compute_coefficient_matrix(query, key, d)\n",
    "\n",
    "second_query = query[0, 2]\n",
    "third_key = key[0, 3]\n",
    "\n",
    "assert torch.allclose(coefficient_matrix[0, 2,3], scalar_similarity_coefficient(second_query, third_key, d))\n",
    "assert torch.allclose(coefficient_matrix[0], coefficient_matrix[1])\n",
    "assert torch.Size([batch_size, t,k]) == coefficient_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-w7SnavXKlfr"
   },
   "outputs": [],
   "source": [
    "def slow_non_vector_compute_coefficient_matrix(query, key, d):\n",
    "    b_s, q_l, _ = query.shape\n",
    "    _, k_l, _ = key.shape\n",
    "    out = torch.zeros(b_s, q_l, k_l)\n",
    "\n",
    "    for b in range(b_s):\n",
    "        for q in range(q_l):\n",
    "            for k in range(k_l):\n",
    "                current_query = query[b, q]\n",
    "                current_key = key[b, k]\n",
    "                out[b, q, k] = scalar_similarity_coefficient(current_query, current_key, d)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NvvDnzb1Mj-H"
   },
   "outputs": [],
   "source": [
    "d = 64\n",
    "t,k = 32, 10\n",
    "batch_size = 128\n",
    "\n",
    "query = torch.rand(batch_size, t, d)\n",
    "key = torch.rand(batch_size, k, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N3dIqxouMcu6",
    "outputId": "e457468d-a457-4eb2-aee9-46a77c43fd74"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "slow_attention_results = slow_non_vector_compute_coefficient_matrix(query, key, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RJDkL_y6MJ9D",
    "outputId": "04be800e-fbdd-4168-fcb2-abe358a2d575"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "fast_attention_result = compute_coefficient_matrix(query, key, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbvmMbxMNOBQ"
   },
   "source": [
    "**Using vectorization significant reduce attention computation time.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xYBGkpLVM1de"
   },
   "outputs": [],
   "source": [
    "assert torch.allclose(slow_attention_results, fast_attention_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XmFMDaC2JCYw"
   },
   "source": [
    "After getting the coefficient matrix, we need to transform it into probabilities by applying the softmax function. Each row contains a coefficient between one query and all keys, which should be a probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9NPQM0n8S19J"
   },
   "outputs": [],
   "source": [
    "## imagine that we have 10 queries and 8 keys and need to construce matrix of cofficent.\n",
    "\n",
    "torch.manual_seed(2)\n",
    "d = 64\n",
    "t,k = 10, 8\n",
    "batch_size = 3\n",
    "query = torch.rand(batch_size, 10, d)\n",
    "key = torch.rand(batch_size, 8, d)\n",
    "\n",
    "def compute_attention_matrix(query, key, d):\n",
    "    coefficient_matrix = compute_coefficient_matrix(query, key, d)    \n",
    "    ## TODO apply softmax\n",
    "    attention_matrix = torch.softmax(coefficient_matrix, dim=-1)\n",
    "    return attention_matrix\n",
    "\n",
    "A = compute_attention_matrix(query, key, d)\n",
    "\n",
    "assert np.allclose((A.sum(axis=-1) - 1).sum().numpy(), 0, atol=1e-6)\n",
    "assert torch.all(A >= 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HpHASSZrXd5A"
   },
   "source": [
    "We could visualize the attention matrix where several rows correspond to the query and column number to the key, and the value shows the attention coefficient between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 625
    },
    "id": "6GLsifSxXF2D",
    "outputId": "f51ce699-c2f3-4e65-c25d-7d53549ef787"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.title('Attention matrix')\n",
    "plt.imshow(A[0])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RGmcCmwKQmUV"
   },
   "source": [
    "In this image attention, distribution should be almost uniform as we randomly initialize all elements, and the attention coefficient should be similar for all elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4WmOWsPKXZjJ"
   },
   "source": [
    "The last step is to use attention weights to compute the weighted sum of values. Sum each value with coefficient taken from the attention matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pgNq6vIHjBTj"
   },
   "source": [
    "\\begin{align}\n",
    " O_k = \\sum^n_{i = 1} a_{k i} v_i = A_k V\n",
    "\\end{align}\n",
    " where k - is a number of attention output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5LPCLwDjclQi"
   },
   "outputs": [],
   "source": [
    "## imagine that we have 10 queries and 8 keys and need to construce matrix of cofficent.\n",
    "\n",
    "torch.manual_seed(6)\n",
    "d = 64\n",
    "t,k = 10, 8\n",
    "batch_size = 3\n",
    "query = torch.rand(batch_size, t, d)\n",
    "key = torch.rand(batch_size, k, d)\n",
    "value = torch.rand(batch_size, k, d)\n",
    "\n",
    "def attention(query, key, value, d):\n",
    "    coefficient_matrix = compute_coefficient_matrix(query, key, d)    \n",
    "    attention_matrix = torch.softmax(coefficient_matrix, dim=-1)\n",
    "    out = attention_matrix @ value\n",
    "    return attention_matrix, out \n",
    "\n",
    "A, out = attention(query, key, value, d)\n",
    "assert out.shape == query.shape\n",
    "assert torch.allclose(out.sum(), torch.tensor(973.154541015625))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JoLk0NtMvvZL"
   },
   "source": [
    "### Attention layer\n",
    "To help attention find interesting patterns in data, we use linear projections to create queries, keys and values. A model can train such a projection matrix and describe data in the best way to solve the problem.\n",
    "\n",
    "\\begin{align}\n",
    "q = w_q x_q \\\\\n",
    "k = w_k x_k \\\\\n",
    "v = w_v x_v \\\\\n",
    "\\end{align}\n",
    "\n",
    "The following steps are the same with attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PheVTmH-4LIS"
   },
   "outputs": [],
   "source": [
    "class AttentionLayer(torch.nn.Module):\n",
    "    def __init__(self, d):\n",
    "        super().__init__()\n",
    "        self.q_layer = torch.nn.Linear(d, d)\n",
    "        self.k_layer = torch.nn.Linear(d, d, bias=False)\n",
    "        self.v_layer = torch.nn.Linear(d, d)\n",
    "        self.d = d\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        self.q_proj = self.q_layer(query)\n",
    "        self.k_proj = self.k_layer(key)\n",
    "        self.v_proj = self.v_layer(value)        \n",
    "        self.A, out = attention(self.q_proj, self.k_proj, self.v_proj, self.d)\n",
    "        return out\n",
    "\n",
    "d = 32\n",
    "t,k = 10, 8\n",
    "batch_size = 3\n",
    "query = torch.rand(batch_size, t, d)\n",
    "key = torch.rand(batch_size, k, d)\n",
    "value = torch.rand(batch_size, k, d)\n",
    "\n",
    "\n",
    "att_layer_out = AttentionLayer(32)(query, key, value)\n",
    "assert att_layer_out.shape == query.shape\n",
    "assert not torch.allclose(attention(query, key, value, d)[1], att_layer_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Oa9nuOE3Lmb"
   },
   "source": [
    "### Self-attention\n",
    "Attention mechanism does not suggest any restriction for input data, and a large number of modern architectures use attention when all $x_q, x_k, x_v$ the same input data. In this case query, key, the value produced by the same source but after application of trainable projection matrix represent data from a different point of view.\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    " x_q = x_k = x_v = x\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "q = w_q x \\\\\n",
    "k = w_k x \\\\\n",
    "v = w_v x \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c_IHVnqFv0_-"
   },
   "outputs": [],
   "source": [
    "d = 32\n",
    "t = 10\n",
    "batch_size = 3\n",
    "x = torch.rand(batch_size, t, d)\n",
    "\n",
    "att_layer_out = AttentionLayer(32)(x, x, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4xi5s5wziV7e"
   },
   "source": [
    "# Let's train models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yMY-5JZIjWoQ"
   },
   "source": [
    "Let's consider toy examples to see how the attention mechanism work.\n",
    "Our dataset is a sequence where each object represents a real number and flag. Our task is to find the sum of elements where the flag equals 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z7rpmKkqjRwg"
   },
   "outputs": [],
   "source": [
    "def generate_dataset(seq_len, sample_count, target_elements_per_seq=3):\n",
    "    marker = torch.zeros(sample_count, seq_len)\n",
    "    for i in range(sample_count):\n",
    "        pos = np.random.permutation(np.arange(seq_len))[:target_elements_per_seq]        \n",
    "        marker[i][pos] = 1.\n",
    "    scaler = 10\n",
    "    number = torch.rand(sample_count, seq_len) * scaler - scaler / 2\n",
    "    y = (number * marker).sum(axis=1)\n",
    "    x = torch.stack((number, marker), dim=-1)\n",
    "    return x,y \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lw8FJIDjOlxZ"
   },
   "outputs": [],
   "source": [
    "## generating data\n",
    "## [\n",
    "##    (x_1, flag_1), (x_2, flag_2) ... (x_n, flag_n)\n",
    "## ]\n",
    "x,y = generate_dataset(20, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YEa_I4Et0Y7b"
   },
   "outputs": [],
   "source": [
    "## split data into train and test\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1PhDVbg60J6F"
   },
   "outputs": [],
   "source": [
    "## constructing train and validation\n",
    "train_dataset = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "test_dataset = torch.utils.data.TensorDataset(x_test, y_test)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=2)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1024, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4VcnbZfx1X8E"
   },
   "source": [
    "To construct keys and values, we use input data and then apply a projection matrix. We utilise one trainable query to filter our data (such construction equals a row of attention matrix). Then we compute attention between all keys and one query with attention layer and use the obtained coefficient to make a weighted sum of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S9vLfH5K0_3c"
   },
   "outputs": [],
   "source": [
    "class SummationAttentionModel(torch.nn.Module):\n",
    "    def __init__(self, attention_layer, d):\n",
    "        super().__init__()\n",
    "        ## trainiable query\n",
    "        self.syntetic_query = torch.nn.Parameter(torch.rand(1, 1, d))\n",
    "        ## init layer for attention operation\n",
    "        self.attention_layer = attention_layer\n",
    "        ## we have one q in R^(d) -> output R^(d)\n",
    "        ## linear layer map it in single output\n",
    "        self.output_layer = torch.nn.Linear(d, 1)\n",
    "\n",
    "    def forward(self, x):    \n",
    "        ## make attention between syntetic_query and x\n",
    "        out = self.attention_layer(self.syntetic_query, x, x)        \n",
    "        ## out shape [n, 1, d] -> remove one dimension\n",
    "        out = out[:, 0, :]        \n",
    "        ##  [n, d] -> [n, 1] for solving regression task\n",
    "        prediction = self.output_layer(out)[:, 0]\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M4Mpk1MK7xKC"
   },
   "outputs": [],
   "source": [
    "d = 2\n",
    "## init model for our dataset with dimension size == 2\n",
    "attention_layer = AttentionLayer(d)\n",
    "model = SummationAttentionModel(attention_layer, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fOf5GaAd8TnD"
   },
   "outputs": [],
   "source": [
    "## model use MSE loss with adam optimizer with default parameters\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "a5f4075a87a04dbc8937925b0e2941fb",
      "2b964dee2119432ea27b6ca17fa603ea",
      "01ec6b56cc564c34b04bece8413dd465",
      "6f7436fbd7074e348b996368ea44d33c",
      "77c177a079054ced9dc9cd21deb3316f",
      "95c64a6fb6084cb8b033c31f7dd7550d",
      "f9bef0913bd24a568ecc9c1ba3cfab46",
      "6a91a3fff83a4f15bd8b19f0fc81de19",
      "7b4b2e1a347d4c29bb5734456cb60819",
      "f47555bb8c97494db3610ea0da8ed410",
      "e3717f0b5847484e960b407b7c8120a7"
     ]
    },
    "id": "BuOBOsEn8MgB",
    "outputId": "c3d28049-8487-4be2-deb4-9d759378e054"
   },
   "outputs": [],
   "source": [
    "model.train()\n",
    "losses_by_epoch = []\n",
    "## train model during 100 epochs\n",
    "for epoch in tqdm(range(50)):\n",
    "    epoch_loss = []\n",
    "    for batch in train_dataloader:\n",
    "        x,y = batch\n",
    "        ## take input data and target\n",
    "        pred = model(x)\n",
    "        ## compute loss function and calculate gradients\n",
    "        loss = criterion(y, pred)\n",
    "        loss.backward()\n",
    "        \n",
    "        ## weights update\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        ## store loss for future plots\n",
    "        epoch_loss.append(loss.detach().item())            \n",
    "    losses_by_epoch.append(np.mean(epoch_loss))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6peYFypDHlg"
   },
   "source": [
    "Training loss during epoch should decrease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "id": "Ged6FQUBG17g",
    "outputId": "448f273a-7c67-4b83-ce97-dc9236549df0"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title('Train loss')\n",
    "plt.yscale('log')\n",
    "plt.plot(losses_by_epoch);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "alx-q-T1Jn4r"
   },
   "source": [
    "Attention visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TNJtfxEeHa7s"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "batch = next(iter(test_dataloader))\n",
    "x,y = batch\n",
    "pred = model(x)\n",
    "\n",
    "## save attention matrix for validation examples\n",
    "valid_attention = model.attention_layer.A.detach().numpy()[:, 0]\n",
    "\n",
    "k = model.attention_layer.k_proj.detach().numpy()\n",
    "v = model.attention_layer.v_proj.detach().numpy()\n",
    "q = model.attention_layer.q_proj.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TWilGJ0REa92"
   },
   "source": [
    "Queries for target elements should be a closer query with respect to the zero point to make the scalar product bigger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 446
    },
    "id": "_IpxjlpPJzk4",
    "outputId": "0a6b23ea-a9f8-47bc-c71e-6949fff41047"
   },
   "outputs": [],
   "source": [
    "idx = 5\n",
    "idx_of_target_examples = np.where(x[idx][:, 1] == 1)[0]\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.scatter(k[idx, :, 0], k[idx, :, 1], label='Keys of non-target elements')\n",
    "plt.scatter(k[idx, idx_of_target_examples, 0], k[idx, idx_of_target_examples, 1], label='Keys of target elements')\n",
    "\n",
    "plt.scatter(q[0, :, 0], q[0, :, 1], label='Trainiable query')\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IlimzzFEGMZO"
   },
   "source": [
    "The attention of query is close to zero for non-marked elements in a sequence, but considerable attention equals 1/3 because of softmax normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 355
    },
    "id": "HG1EmdMcHpWo",
    "outputId": "ea1f3518-4bcc-4aa9-e68f-76ef2d0fdd3e"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title('Row of Attention matrix')\n",
    "plt.plot(valid_attention[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jHHhSjctIBY5"
   },
   "source": [
    "For different examples, the situation is the same. There are three peaks for each sequence that corresponds to the marked objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 484
    },
    "id": "xFXiPY9VTIKl",
    "outputId": "35c87a80-be45-4174-a5ac-cf26d560e39f"
   },
   "outputs": [],
   "source": [
    "## attention of differernt examples\n",
    "idxes = [0, 1,10 ,256, 777,228, 412]\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.title('Attention for several sequences')\n",
    "plt.yticks(range(len(idxes)), labels=idxes)\n",
    "plt.ylabel('Sequences number')\n",
    "plt.xlabel('Object number')\n",
    "plt.imshow(valid_attention[idxes])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0CwrBlwfULzH"
   },
   "source": [
    "All plots summarize our understanding of the attention mechanism in training queries and keys to pay more attention to relevant objects and reduce the influence of other objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mR6IYiOKUrNd"
   },
   "outputs": [],
   "source": [
    "## todo visualize values for marked and non marked objets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sj_YzxztQ8Rd"
   },
   "source": [
    "### Masked attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yukumvPzU4uk"
   },
   "source": [
    "There are many datasets where sequence length is different in real life, but we still need batch processing to make a model work fast and efficient. For this purpose, we use padding for a sequence.\n",
    "\n",
    "![padded_sequence.png](https://raw.githubusercontent.com/zaaabik/msd2021/main/padded_sequence.png)\n",
    "\n",
    "Our task is to create a model that pushes the model to ignore padding elements and make attention for them equal to zero.\n",
    "\n",
    "![masked_non_masked_attention.png](https://raw.githubusercontent.com/zaaabik/msd2021/main/masked_non_masked_attention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SJao9WpQWYAh"
   },
   "source": [
    "Your task is:\n",
    "\n",
    "\n",
    "1.   Pad keys and values using \n",
    "`torch.nn.utils.rnn.pad_sequence(sequences, batch_first=False, padding_value=0.0)`. **Use parameter batch_first=True, to make output compatible with our attention function**\n",
    "2.   Create a mask for keys and values, which help you make zeros in the attention matrix. To change the number in the attention matrix, you can use\n",
    "`Tensor.masked_fill_(mask, value). This function takes Tensor and boolean masks with the same shape and then fills tensor elements with value parameters where the mask is True.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r942aj_ARJas"
   },
   "outputs": [],
   "source": [
    "def masked_attention(query, key, value, mask, d):\n",
    "    coefficient_matrix = compute_coefficient_matrix(query, key, d)    \n",
    "    ## todo make an masked attention\n",
    "    return attention_matrix, out\n",
    "\n",
    "d = 64\n",
    "t = 4\n",
    "batch_size = 1\n",
    "query = torch.rand(batch_size, t, d)\n",
    "key = [\n",
    "    torch.rand(4, d),\n",
    "    torch.rand(1, d),\n",
    "    torch.rand(6, d),\n",
    "    torch.rand(3, d),\n",
    "]\n",
    "value = [ \n",
    "    torch.rand(4, d),\n",
    "    torch.rand(1, d),\n",
    "    torch.rand(6, d),\n",
    "    torch.rand(3, d),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mOpegIFNYzHV"
   },
   "outputs": [],
   "source": [
    "## print key and values, and find zeros at the end of short sequences\n",
    "## plot attention matix, to see zeros coefficents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SmcfpgHDNJn"
   },
   "source": [
    "## Evaluate real model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VPgMNGbDDNJn"
   },
   "source": [
    "Based on [NLP example](https://teddykoker.com/2020/02/nlp-from-scratch-annotated-attention/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VUYskM2ZrX8C"
   },
   "source": [
    "Our task is to translate a sentence from German to English. We use the model where the encoder is the LSTM model, and the attention layer is applied as the decoder part. LSTM model generates hidden states and then uses the last hidden state of the predicted token as a query to find attention coefficients. After that model, make a weighted sum of hidden states of LSTM based attention coefficient and compute the next predicted token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6tFBxt8drYsh"
   },
   "source": [
    "![attention-machine-translation](https://raw.githubusercontent.com/zaaabik/msd2021/main/attention.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lj4pVgRabc4q"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K1cMs8pUbdfr"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decode output from hidden state and context\n",
    "    \"\"\"\n",
    "    def __init__(self, output_dim, embed_dim, hidden_dim, num_layers, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.embed = nn.Embedding(output_dim, embed_dim)\n",
    "        # stacking LSTM\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, n_layers, dropout=dropout)\n",
    "        self.attention = Attention(hidden_dim) # we'll get to later\n",
    "        self.wc = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.ws = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, trg, hidden, encoder_out):\n",
    "        trg = trg.unsqueeze(0)\n",
    "        embed = self.dropout(self.embed(trg))\n",
    "        decoder_out, hidden = self.lstm(embed, hidden)\n",
    "        \n",
    "        # we'll go over how these are computed later\n",
    "        atten, context = self.attention(decoder_out, encoder_out)\n",
    "        \n",
    "        # \"We employ a simple concatenation layer to combine the \n",
    "        # information from both vectors:\"\n",
    "        atten_hidden = self.wc(torch.cat((decoder_out, context), dim=2)).tanh()\n",
    "\n",
    "        # \"The attentional vector ~h_t is then fed through the softmax layer\n",
    "        # to produce the predictive distribution:\"\n",
    "        out = self.ws(atten_hidden.squeeze(0))\n",
    "        # softmax will be included in loss function\n",
    "        \n",
    "        return out, hidden, atten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tHYROm2LbidD"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Compute alignment vector and context vector from hidden states\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim, score_fn=\"general\"):\n",
    "        super(Attention, self).__init__()\n",
    "        self.score_fn = score_fn\n",
    "        if score_fn == \"general\":\n",
    "            self.w = nn.Linear(hidden_dim, hidden_dim)\n",
    "        if score_fn == \"concat\":\n",
    "            self.w = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "            self.v = torch.nn.Parameter(torch.FloatTensor(hidden_dim))\n",
    "            \n",
    "    def score(self, decoder_out, encoder_outs):\n",
    "        if self.score_fn == \"dot\":\n",
    "            return torch.sum(decoder_out * encoder_outs, dim=2)\n",
    "        if self.score_fn == \"general\":\n",
    "            return torch.sum(decoder_out * self.w(encoder_outs), dim=2)\n",
    "        if self.score_fn == \"concat\":\n",
    "            decoder_outs = decoder_out.repeat(encoder_outs.shape[0], 1, 1)\n",
    "            cat = torch.cat((decoder_outs, encoder_outs))\n",
    "            return torch.sum(self.v * self.w(cat), dim=2)\n",
    "            \n",
    "    def forward(self, decoder_out, encoder_outs):\n",
    "        score = self.score(decoder_out, encoder_outs)\n",
    "        a = F.softmax(score, dim=0)\n",
    "            \n",
    "        # \"Given the alignment vector as weights, the context vector \n",
    "        # c_t is computed as the weighted average over all the source \n",
    "        # hidden states:\"\n",
    "        context = torch.bmm(\n",
    "            a.transpose(1, 0).unsqueeze(1),\n",
    "            encoder_outs.transpose(1, 0)\n",
    "        ).transpose(1, 0)\n",
    "        return a, context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8muNqIdgboN8"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Stacked LSTM encoder\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, embed_dim, hidden_dim, \n",
    "                 n_layers, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed = nn.Embedding(input_dim, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, n_layers, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout) \n",
    "        \n",
    "    def forward(self, src):\n",
    "        embed = self.dropout(self.embed(src))\n",
    "        out, hidden = self.lstm(embed)\n",
    "        return self.dropout(out), hidden\n",
    "    \n",
    "\n",
    "class Model(nn.Module):\n",
    "    \"\"\"\n",
    "    Sequence to Sequence model with attention\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Model, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, src, trg, teacher_force_ratio = 0.5):\n",
    "        outs = torch.zeros(\n",
    "            trg.shape[0], trg.shape[1], self.decoder.output_dim\n",
    "        ).to(src.device)\n",
    "        encoder_out, hidden = self.encoder(src)\n",
    "        \n",
    "        x = trg[0]\n",
    "        for t in range(1, trg.shape[0]):\n",
    "            outs[t], hidden, _ = self.decoder(x, hidden, encoder_out)\n",
    "            x = trg[t] if random.random() < teacher_force_ratio else outs[t].argmax(1)\n",
    "            \n",
    "        return outs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8r4Oo0dbDNJp"
   },
   "source": [
    "We choose a relatively small dataset with 30 thousand English-German sentence pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GTbPhb1icH9H"
   },
   "outputs": [],
   "source": [
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DavOzYH-dA7L",
    "outputId": "56b5d407-a04e-4f62-cb94-92b0e79270c4"
   },
   "outputs": [],
   "source": [
    "## download dataset\n",
    "!python -m spacy download en\n",
    "!python -m spacy download de"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zioZERAkDNJq"
   },
   "source": [
    "Common library torchtext includes a lot of powerful tools for text preprocessing and often appears in different NLP tasks. For our purpose, first of all, we initialize a ready tokenizer. Then put it in the special class Field (\"analogue\" of torch's Dataset) with other instructions for converting text to Tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AMBpDm8TbsGW",
    "outputId": "e026edfd-b4aa-49a9-a8f9-7deefc4c6030"
   },
   "outputs": [],
   "source": [
    "from torchtext.legacy.datasets import Multi30k\n",
    "from torchtext.legacy.data import Field, BucketIterator\n",
    "import spacy\n",
    "\n",
    "# tokenizers\n",
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')\n",
    "tokenize_de = lambda text: [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "tokenize_en = lambda text: [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "# fields\n",
    "SRC = Field(tokenize=tokenize_de, init_token='<sos>',\n",
    "            eos_token=\"<eos>\", lower=True)\n",
    "TRG = Field(tokenize=tokenize_de, init_token='<sos>',\n",
    "            eos_token=\"<eos>\", lower=True)\n",
    "\n",
    "# data\n",
    "train_data, valid_data, test_data = Multi30k.splits(('.de', '.en'), (SRC, TRG))\n",
    "SRC.build_vocab(train_data, min_freq=2)\n",
    "TRG.build_vocab(train_data, min_freq=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QXJFz1HoDNJs"
   },
   "source": [
    "As we use a relatively small set of data, we can initialize a pretty simple model. The training time from scratch is ~30 minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uOy9lc21dLHc"
   },
   "outputs": [],
   "source": [
    "input_dim = len(SRC.vocab)\n",
    "output_dim = len(TRG.vocab)\n",
    "embed_dim = 256\n",
    "hidden_dim = 512\n",
    "n_layers = 2\n",
    "dropout = 0.2\n",
    "batch_size = 128\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def init_weights(model):\n",
    "    for param in model.parameters():\n",
    "        nn.init.uniform_(param.data, -0.1, 0.1)\n",
    "\n",
    "train_loader, valid_loader, test_loader = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size=batch_size,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "encoder = Encoder(input_dim, embed_dim, hidden_dim, n_layers, dropout)\n",
    "\n",
    "decoder = Decoder(output_dim, embed_dim, hidden_dim, n_layers, dropout)\n",
    "model = Model(encoder, decoder).to(device)\n",
    "model.apply(init_weights);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e_3N2-5MpyLp"
   },
   "outputs": [],
   "source": [
    "def step(model, data, criterion, train=False, optimizer=None):\n",
    "    model.train() if train else model.eval()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(data, leave=False):\n",
    "        if train: optimizer.zero_grad()\n",
    "        pred = model(batch.src, batch.trg)\n",
    "        loss = criterion(pred.view(-1, pred.size(2)), batch.trg.view(-1))\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6rx4fN-6bxRS"
   },
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.Adam(model.parameters())\n",
    "# criterion = nn.CrossEntropyLoss(ignore_index=TRG.vocab.stoi[TRG.pad_token])\n",
    "# num_epochs = 10\n",
    "\n",
    "\n",
    "\n",
    "# best_loss = float('inf')\n",
    "# train_loss, valid_loss = np.zeros((2, num_epochs))\n",
    "# for e in range(num_epochs):\n",
    "#     train_loss[e] = step(model, train_loader, criterion, train=True, optimizer=optimizer)\n",
    "#     valid_loss[e] = step(model, valid_loader, criterion)\n",
    "#     #print(f\"epoch: {e} train_loss: {train_loss[e]:.2f} valid_loss: {valid_loss[e]:.2f}\")\n",
    "#     if valid_loss[e] < best_loss:\n",
    "#         torch.save(model.state_dict(), 'model.pt')\n",
    "#         best_loss = valid_loss[e]\n",
    "\n",
    "# plt.figure(figsize=(5, 3), dpi=300)\n",
    "# plt.plot(train_loss[:10], label=\"Train\")\n",
    "# plt.plot(valid_loss[:10], label=\"Valid.\")\n",
    "# plt.legend(); plt.ylabel(\"Cross-Entropy Loss\"); plt.xlabel(\"Epoch\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bIOZg45bDNJt"
   },
   "source": [
    "Of course, we can't wait this time, so we upload pre-trained weights. The interested listener can try to train the model by himself (uncomment above):) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53,
     "referenced_widgets": [
      "15e181e98a8a4f97b3c3873b9feed060",
      "5aeaffa904664faa8d87a85b9a554a70",
      "b2e36117c6814779826d6b4c9a7a026a",
      "648e26d49f3b4731a44ee46fd381a63d",
      "7e754f7361164fa58d4ca227de717a17",
      "b2fc38679a84465a832437b20cfef0f8",
      "134c1b54690540bebf9b0dad77ce0fdd",
      "76777fdb8a3447a889e35208b2516269",
      "5463299501704ae4aa7a45329f58ad2a",
      "d2ee99e808cf426298335092bebe8682",
      "788bf8c091424a98b5b78ecdf8e129f7"
     ]
    },
    "id": "et1av3otenxL",
    "outputId": "d40846e6-6e4c-41fa-907b-2bbbf5822afa"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=TRG.vocab.stoi[TRG.pad_token])\n",
    "model.load_state_dict(torch.load(\"model.pt\"))\n",
    "test_loss = step(model, test_loader, criterion)\n",
    "print(f\"Test loss: {test_loss:.2f}\")\n",
    "print(f\"Test perplexity: {np.exp(test_loss):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cWsbXgc5DNJu"
   },
   "source": [
    "**Let's translate!** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dbzz1gP0eqgq"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def translate(sentence, model, device, max_len=50):\n",
    "    model.eval()\n",
    "    src = SRC.process([sentence]).to(device)\n",
    "    trg = torch.ones(1, dtype=torch.int64).to(device) * TRG.vocab.stoi[TRG.init_token]\n",
    "    trgs, attention = [], []\n",
    "    encoder_out, hidden = model.encoder(src)\n",
    "    \n",
    "    for t in range(max_len):\n",
    "        trg, hidden, atten = model.decoder(trg, hidden, encoder_out)\n",
    "        trg = trg.argmax(1)\n",
    "        trgs.append(trg)\n",
    "        attention.append(atten.T)\n",
    "        if trg == TRG.vocab.stoi[TRG.eos_token]: break\n",
    "\n",
    "    trg = [TRG.vocab.itos[i] for i in trgs]\n",
    "    src = [SRC.vocab.itos[i] for i in src]\n",
    "    attention = torch.cat(attention).cpu().numpy()[:-1, 1:]\n",
    "    return src, trg, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "orBmrNuTetFX",
    "outputId": "b6eb2664-31d7-4b17-b2c2-16841f91013a"
   },
   "outputs": [],
   "source": [
    "example = 6\n",
    "src, trg = test_data[example].src, test_data[example].trg\n",
    "print(f\"Source: {' '.join(src)}\")\n",
    "print(f\"Target: {' '.join(trg)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7BtMcxEMkxS8",
    "outputId": "15195f01-16aa-4932-9908-16bc6b59f93c"
   },
   "outputs": [],
   "source": [
    "src, pred, attention = translate(src, model, device)\n",
    "print(f\"Prediction: {' '.join(pred[:-1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2CNG2Jv-DNJv"
   },
   "source": [
    "![igloo.png](https://lh3.googleusercontent.com/proxy/pnwe3oLkqNRjlKF3U5cMIzvtouLACzR1jW5oRZTMJXf13ZSiQ3w2924fgzM4RjV3NU-29xYpJ4jPHBOVpYFb_W0T-6VzPcATw853Y5iXljk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 738
    },
    "id": "Efs1UwIae3QF",
    "outputId": "7e7724f6-c9c4-4128-a6f1-18d2a7469a4a"
   },
   "outputs": [],
   "source": [
    "def plot_attention(src, trg, attention):\n",
    "    fig = plt.figure(figsize=(5, 5), dpi=150)\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.matshow(attention)\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.set_xticklabels([''] + src, rotation=60)\n",
    "    ax.set_yticklabels([''] + trg) \n",
    "    \n",
    "plot_attention(src, pred, attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f7gpaIw2nLFt"
   },
   "outputs": [],
   "source": [
    "src = 'Mann im schwarzen Anzug geht zur Bushaltestelle'.lower().split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lRqMMSGfnB-M",
    "outputId": "0da2515c-507c-4214-dc3f-ddd8bbce5258"
   },
   "outputs": [],
   "source": [
    "src, pred, attention = translate(src, model, device)\n",
    "print(f\"Prediction: {' '.join(pred[:-1])}\")\n",
    "print(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 771
    },
    "id": "pps8Waw0pSJV",
    "outputId": "916472a4-383d-4489-cca5-c41924baa58a"
   },
   "outputs": [],
   "source": [
    "plot_attention(src, pred, attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uyX13FSkDNJw"
   },
   "source": [
    "![man_in_a_black](https://i.kym-cdn.com/entries/icons/original/000/027/208/men_in_black_3_still.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vOvee4f8u1Ou"
   },
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IULrQ5Lcu5vn"
   },
   "source": [
    "*   Query, key helps the model find a relevant object in a sequence + interpretable.\n",
    "*   Computing attention matrix in a vectorize form significant improve the speed.\n",
    "*   For batch processing with sequences with different lengths, masked attention should be used.\n",
    "*   Attention mechanism help model directly use objects on a sequence and improve quality of RNN models.\n",
    "*   There are several functions used as the score function."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "attention_seminar_v7_ER.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "01ec6b56cc564c34b04bece8413dd465": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f9bef0913bd24a568ecc9c1ba3cfab46",
      "placeholder": "​",
      "style": "IPY_MODEL_95c64a6fb6084cb8b033c31f7dd7550d",
      "value": "100%"
     }
    },
    "134c1b54690540bebf9b0dad77ce0fdd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "15e181e98a8a4f97b3c3873b9feed060": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b2e36117c6814779826d6b4c9a7a026a",
       "IPY_MODEL_648e26d49f3b4731a44ee46fd381a63d",
       "IPY_MODEL_7e754f7361164fa58d4ca227de717a17"
      ],
      "layout": "IPY_MODEL_5aeaffa904664faa8d87a85b9a554a70"
     }
    },
    "2b964dee2119432ea27b6ca17fa603ea": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5463299501704ae4aa7a45329f58ad2a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5aeaffa904664faa8d87a85b9a554a70": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "648e26d49f3b4731a44ee46fd381a63d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5463299501704ae4aa7a45329f58ad2a",
      "max": 8,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_76777fdb8a3447a889e35208b2516269",
      "value": 8
     }
    },
    "6a91a3fff83a4f15bd8b19f0fc81de19": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6f7436fbd7074e348b996368ea44d33c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7b4b2e1a347d4c29bb5734456cb60819",
      "max": 15,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6a91a3fff83a4f15bd8b19f0fc81de19",
      "value": 15
     }
    },
    "76777fdb8a3447a889e35208b2516269": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "77c177a079054ced9dc9cd21deb3316f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e3717f0b5847484e960b407b7c8120a7",
      "placeholder": "​",
      "style": "IPY_MODEL_f47555bb8c97494db3610ea0da8ed410",
      "value": " 15/15 [00:16&lt;00:00,  1.09s/it]"
     }
    },
    "788bf8c091424a98b5b78ecdf8e129f7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7b4b2e1a347d4c29bb5734456cb60819": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7e754f7361164fa58d4ca227de717a17": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_788bf8c091424a98b5b78ecdf8e129f7",
      "placeholder": "​",
      "style": "IPY_MODEL_d2ee99e808cf426298335092bebe8682",
      "value": " 8/8 [00:01&lt;00:00,  6.58it/s]"
     }
    },
    "95c64a6fb6084cb8b033c31f7dd7550d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a5f4075a87a04dbc8937925b0e2941fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_01ec6b56cc564c34b04bece8413dd465",
       "IPY_MODEL_6f7436fbd7074e348b996368ea44d33c",
       "IPY_MODEL_77c177a079054ced9dc9cd21deb3316f"
      ],
      "layout": "IPY_MODEL_2b964dee2119432ea27b6ca17fa603ea"
     }
    },
    "b2e36117c6814779826d6b4c9a7a026a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_134c1b54690540bebf9b0dad77ce0fdd",
      "placeholder": "​",
      "style": "IPY_MODEL_b2fc38679a84465a832437b20cfef0f8",
      "value": "100%"
     }
    },
    "b2fc38679a84465a832437b20cfef0f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d2ee99e808cf426298335092bebe8682": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e3717f0b5847484e960b407b7c8120a7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f47555bb8c97494db3610ea0da8ed410": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f9bef0913bd24a568ecc9c1ba3cfab46": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
